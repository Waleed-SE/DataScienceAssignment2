{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_files(raw_folder, output_file):\n",
    "    electricity_folder = os.path.join(raw_folder, \"electricity_raw_data\")\n",
    "    weather_folder = os.path.join(raw_folder, \"weather_raw_data\")\n",
    "\n",
    "    electricity_files = [os.path.join(electricity_folder, f) for f in os.listdir(electricity_folder) if f.endswith('.json')]\n",
    "    weather_files = [os.path.join(weather_folder, f) for f in os.listdir(weather_folder) if f.endswith('.csv')]\n",
    "\n",
    "    electricity_data = []\n",
    "    weather_data = []\n",
    "\n",
    "    for file in electricity_files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = pd.json_normalize(pd.read_json(f)['response']['data'])\n",
    "            data['datetime'] = pd.to_datetime(data['period'], format='%Y-%m-%dT%H')\n",
    "            data.drop(columns=['period'], inplace=True)\n",
    "            electricity_data.append(data)\n",
    "\n",
    "    for file in weather_files:\n",
    "        data = pd.read_csv(file)\n",
    "        data.rename(columns={'date': 'datetime'}, inplace=True)\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
    "        weather_data.append(data)\n",
    "\n",
    "    if electricity_data and weather_data:\n",
    "        electricity_df = pd.concat(electricity_data, ignore_index=True)\n",
    "        weather_df = pd.concat(weather_data, ignore_index=True)\n",
    "\n",
    "        if weather_df['datetime'].dtype == 'object':\n",
    "            weather_df['datetime'] = pd.to_datetime(weather_df['datetime'], errors='coerce')\n",
    "\n",
    "        weather_df['datetime'] = weather_df['datetime'].dropna().dt.tz_localize(None)\n",
    "\n",
    "        merged_df = pd.merge(electricity_df, weather_df, on='datetime', how='inner')\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged file saved at: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files found in raw folder.\")\n",
    "\n",
    "# Example usage\n",
    "raw_folder_path = \"raw\"  # Update this to the actual path\n",
    "output_file_path = \"merged_data.csv\"\n",
    "merge_files(raw_folder_path, output_file_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "input_file = \"merged_data.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Remove leading/trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Log original record count\n",
    "print(f\"Total records before cleaning: {df.shape[0]}\")\n",
    "\n",
    "# 1. Identifying Missing Data\n",
    "missing_data = df.isnull().sum() / len(df) * 100\n",
    "print(\"Missing Data (%):\\n\", missing_data)\n",
    "\n",
    "# 3. Handling Missing Data\n",
    "threshold = 50  # Drop columns with >50% missing data\n",
    "df = df.dropna(axis=1, thresh=len(df) * (threshold / 100))\n",
    "\n",
    "# Fill missing values in numerical columns with the median\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# 4. Data Type Conversions\n",
    "# Convert 'datetime' column to datetime format\n",
    "if \"datetime\" in df.columns:\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Ensure numerical columns are properly cast\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# 5. Handling Duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate records: {duplicate_count}\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Total records after removing duplicates: {df.shape[0]}\")\n",
    "\n",
    "# 6. Feature Engineering\n",
    "if \"datetime\" in df.columns:\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    # Remove 'season' categorical column\n",
    "    # Create separate season binary columns instead\n",
    "    df[\"season_Winter\"] = df[\"month\"].isin([12, 1, 2]).astype(int)\n",
    "    df[\"season_Spring\"] = df[\"month\"].isin([3, 4, 5]).astype(int)\n",
    "    df[\"season_Summer\"] = df[\"month\"].isin([6, 7, 8]).astype(int)\n",
    "    df[\"season_Fall\"] = df[\"month\"].isin([9, 10, 11]).astype(int)\n",
    "\n",
    "# 7. One-Hot Encoding with Integer Conversion\n",
    "def one_hot_encode(df, column_name, drop_first=False):\n",
    "    \"\"\" One-hot encodes a categorical column and converts new dummy columns to int. \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        df = pd.get_dummies(df, columns=[column_name], drop_first=drop_first)  # Drop first category if needed\n",
    "        dummy_cols = [col for col in df.columns if column_name in col]  # Find newly created columns\n",
    "        df[dummy_cols] = df[dummy_cols].astype(int)  # Convert to integers\n",
    "    return df\n",
    "\n",
    "# Apply encoding to categorical features\n",
    "df = one_hot_encode(df, \"Province\", drop_first=True)  # Avoid dummy variable trap\n",
    "\n",
    "# 8. Convert \"subba-name\" into True/False Features (Binary Columns)\n",
    "if \"subba-name\" in df.columns:\n",
    "    subba_dummies = pd.get_dummies(df[\"subba-name\"], prefix=\"subba\", dtype=int)\n",
    "    df = pd.concat([df, subba_dummies], axis=1)\n",
    "    df.drop(columns=[\"subba-name\"], inplace=True)  # Drop original column\n",
    "\n",
    "# 9. Convert \"weather_name\" into 4 Separate Binary Columns (No Drop)\n",
    "if \"weather_name\" in df.columns:\n",
    "    weather_dummies = pd.get_dummies(df[\"weather_name\"], prefix=\"weather\", dtype=int)\n",
    "    df = pd.concat([df, weather_dummies], axis=1)\n",
    "    df.drop(columns=[\"weather_name\"], inplace=True)  # Drop original column\n",
    "\n",
    "# 10. Normalizing Numerical Features (Optional)\n",
    "# scaler = StandardScaler()\n",
    "# numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Save cleaned data\n",
    "output_csv = \"cleaned_data.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Cleaned data saved as {output_csv}\")\n"
   ],
   "id": "a65adc425d819acb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
    "    df = pd.read_csv(file_path, parse_dates=['datetime'])\n",
    "    return df\n",
    "\n",
    "def compute_statistics(df):\n",
    "    \"\"\"Compute key statistical metrics for numerical features.\"\"\"\n",
    "    stats_summary = {}\n",
    "\n",
    "    for column in df.select_dtypes(include=[np.number]).columns:\n",
    "        stats_summary[column] = {\n",
    "            'Mean': np.mean(df[column]),\n",
    "            'Median': np.median(df[column]),\n",
    "            'Standard Deviation': np.std(df[column], ddof=1),\n",
    "            'Skewness': stats.skew(df[column], nan_policy='omit'),\n",
    "            'Kurtosis': stats.kurtosis(df[column], nan_policy='omit')\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(stats_summary)\n",
    "\n",
    "def plot_time_series(df):\n",
    "    \"\"\"Plot electricity demand over time.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x='datetime', y='value', label=\"Electricity Demand\", color=\"blue\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Electricity Demand (MWh)\")\n",
    "    plt.title(\"Electricity Demand Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def univariate_analysis(df, column):\n",
    "    \"\"\"Performs univariate analysis on a specified numerical column.\"\"\"\n",
    "    if column not in df.columns:\n",
    "        print(f\"Column '{column}' not found in dataset!\")\n",
    "        return\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.histplot(df[column], bins=30, kde=True, ax=axes[0], color='blue')\n",
    "    axes[0].set_title(f'Histogram of {column}')\n",
    "\n",
    "    sns.boxplot(y=df[column], ax=axes[1], color='green')\n",
    "    axes[1].set_title(f'Boxplot of {column}')\n",
    "\n",
    "    sns.kdeplot(df[column], fill=True, color='red', ax=axes[2])\n",
    "    axes[2].set_title(f'Density Plot of {column}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(df[column].describe())\n",
    "    print(f\"\\nSkewness: {df[column].skew():.4f}\")\n",
    "    print(f\"Kurtosis: {df[column].kurt():.4f}\")\n",
    "\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Computes and visualizes the correlation matrix for numerical features.\"\"\"\n",
    "    numerical_df = df.select_dtypes(include=['number'])\n",
    "    correlation_matrix = numerical_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def time_series_analysis(df):\n",
    "    \"\"\"Performs time series decomposition and stationarity test.\"\"\"\n",
    "    df = df.set_index('datetime').resample('H').mean().ffill()\n",
    "\n",
    "    decomposition = seasonal_decompose(df['value'], model='additive', period=24)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(df['value'], label='Original Time Series')\n",
    "    plt.legend()\n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend, label='Trend', color='green')\n",
    "    plt.legend()\n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonality', color='orange')\n",
    "    plt.legend()\n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid, label='Residuals', color='red')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nPerforming Augmented Dickey-Fuller Test...\")\n",
    "    adf_test = adfuller(df['value'].dropna())\n",
    "    print(f\"ADF Statistic: {adf_test[0]:.4f}\")\n",
    "    print(f\"p-value: {adf_test[1]:.4f}\")\n",
    "    for key, value in adf_test[4].items():\n",
    "        print(f\"Critical Value ({key}): {value:.4f}\")\n",
    "    if adf_test[1] < 0.05:\n",
    "        print(\"\\nConclusion: The time series is stationary.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: The time series is NOT stationary.\")\n",
    "\n",
    "def main():\n",
    "    file_path = 'cleaned_data.csv'\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    print(\"Statistical Summary:\")\n",
    "    print(compute_statistics(df))\n",
    "\n",
    "    plot_time_series(df)\n",
    "\n",
    "    numerical_columns = ['value', 'temperature_2m', 'hour', 'day', 'month', 'year', 'weekday', 'is_weekend']\n",
    "    for col in numerical_columns:\n",
    "        print(f\"\\n{'='*40}\\nUnivariate Analysis: {col}\\n{'='*40}\")\n",
    "        univariate_analysis(df, col)\n",
    "\n",
    "    correlation_analysis(df)\n",
    "\n",
    "    time_series_analysis(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a69839540d7ad2cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Loads dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Removes duplicate rows from the dataset.\"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def normalize_value_column(df):\n",
    "    \"\"\"Applies log transformation to normalize the 'value' column, handling negatives and NaNs.\"\"\"\n",
    "    if 'value' in df.columns:\n",
    "        df['value'] = np.log1p(df['value'].clip(lower=0)).fillna(0)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    file_path = \"cleaned_data.csv\"  # Update this with the actual path\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    # Normalize the 'value' column\n",
    "    df = normalize_value_column(df)\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    df.to_csv(\"cleaned_normalized_data.csv\", index=False)\n",
    "    print(\"Cleaned dataset with normalized 'value' column saved as 'cleaned_normalized_data.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "eacf40cb683a2e68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def train_regression_model(df):\n",
    "    \"\"\"Trains a linear regression model, conducts residual analysis, and plots residuals.\"\"\"\n",
    "\n",
    "    # Base features\n",
    "    base_features = ['hour', 'day', 'month', 'weekday', 'temperature_2m']\n",
    "\n",
    "    # Seasonal features\n",
    "    season_features = ['season_Winter', 'season_Spring', 'season_Summer', 'season_Fall']\n",
    "\n",
    "    # Subba features (all columns that start with 'subba_')\n",
    "    subba_features = [col for col in df.columns if col.startswith('subba_')]\n",
    "\n",
    "    # Combine all features\n",
    "    features = base_features + season_features + subba_features\n",
    "    target = 'value'\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if not all(col in df.columns for col in features + [target]):\n",
    "        print(\"Some required columns are missing in the dataset.\")\n",
    "        return\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Handling missing values (fill missing data with column means)\n",
    "    X = X.fillna(X.mean())\n",
    "    y = y.fillna(y.mean())\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute residuals\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "    print(f'R² Score: {r2}')\n",
    "\n",
    "    # 1️⃣ Plot Actual vs. Predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, edgecolor=\"k\")\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # Identity line\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Actual vs. Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2️⃣ Residuals Plot (Residuals vs. Predicted)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.6, edgecolor=\"k\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--')  # Zero reference line\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residuals vs. Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 3️⃣ Histogram of Residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')  # Zero reference line\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    file_path = \"cleaned_normalized_data.csv\"  # Update this with the actual file path\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # Train and evaluate regression model\n",
    "    train_regression_model(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "23f69ba05edc7f55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
